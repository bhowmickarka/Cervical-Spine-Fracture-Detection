{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom glob import glob\nimport pydicom\nimport tensorflow as tf\nimport tqdm as tqdm\nimport tensorflow_io as tfio\nimport pathlib\n\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\"\"\"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\"\"\"\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":5.477112,"end_time":"2022-08-01T23:24:50.070980","exception":false,"start_time":"2022-08-01T23:24:44.593868","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-19T22:16:36.894059Z","iopub.execute_input":"2022-08-19T22:16:36.895323Z","iopub.status.idle":"2022-08-19T22:16:38.841486Z","shell.execute_reply.started":"2022-08-19T22:16:36.895269Z","shell.execute_reply":"2022-08-19T22:16:38.840176Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"\"\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""},"metadata":{}}]},{"cell_type":"code","source":"!pip install -q ../input/for-pydicom/pylibjpeg-1.4.0-py3-none-any.whl\n!pip install -q ../input/for-pydicom/python_gdcm-3.0.14-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -q ../input/for-pydicom/pylibjpeg_libjpeg-1.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2022-08-19T22:14:13.599715Z","iopub.execute_input":"2022-08-19T22:14:13.600455Z","iopub.status.idle":"2022-08-19T22:14:42.426819Z","shell.execute_reply.started":"2022-08-19T22:14:13.600343Z","shell.execute_reply":"2022-08-19T22:14:42.425494Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Creating the Dataset tf.Dataset.from_tensor_slices #","metadata":{"papermill":{"duration":0.003234,"end_time":"2022-08-01T23:24:50.076828","exception":false,"start_time":"2022-08-01T23:24:50.073594","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class cl_CreatingDataset:\n    def __init__(self, imageHeight, imageWidth, batch_size):\n        self.imageHeight = imageHeight\n        self.imageWidth = imageWidth\n        self.batch_size = batch_size\n\n    \n    def creatingPathList(self, trainImagesPath, trainDfPath):\n        trainDf = pd.read_csv(trainDfPath)\n        trainImagesPathList = []\n        labels = []\n        # reading the tain CSV\n        trainDf = pd.read_csv(trainDfPath)\n        for i in tqdm.tqdm(range(len(trainDf))):\n            folderName = trainDf[\"StudyInstanceUID\"].iloc[i]\n            imageFolderPath = os.path.join(trainImagesPath, folderName)\n\n            for file in glob(os.path.join(imageFolderPath, \"*.dcm\")):\n                # taking the imageName\n                trainImagesPathList.append(file)\n                # creating the labels\n                label = np.array([trainDf[\"C1\"].iloc[i],trainDf[\"C2\"].iloc[i], trainDf[\"C3\"].iloc[i], trainDf[\"C4\"].iloc[i], trainDf[\"C5\"].iloc[i], \n                                 trainDf[\"C6\"].iloc[i], trainDf[\"C7\"].iloc[i], trainDf[\"patient_overall\"].iloc[i]])\n                labels.append(label)\n        \n        return trainImagesPathList, labels\n    \n    def parse_function(self, filename, label):\n        image_bytes = tf.io.read_file(filename)\n        image = tfio.image.decode_dicom_image(image_bytes, dtype=tf.float32)\n        #image = tf.image.convert_image_dtype(image, tf.float32)\n        resized_image = tf.image.resize(image, [self.imageHeight, self.imageWidth])\n        compressedImage = tf.squeeze(resized_image, axis = 0)\n        finalImage = tf.repeat(compressedImage, repeats=[3], axis = 2)\n        return finalImage, label\n        \n    def train_preprocess(self, image, label):\n        return image, label\n\n\n    def creatingDataset(self, filenames, labels):\n        dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n        dataset = dataset.shuffle(len(filenames))\n        dataset = dataset.map(self.parse_function, num_parallel_calls=4)\n        dataset = dataset.map(self.train_preprocess, num_parallel_calls=4)\n        dataset = dataset.batch(self.batch_size)\n        dataset = dataset.prefetch(1)\n\n        return dataset","metadata":{"papermill":{"duration":0.01753,"end_time":"2022-08-01T23:24:50.096873","exception":false,"start_time":"2022-08-01T23:24:50.079343","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Dataset using Generator Concept (tf.data.Dataset.fromgenerator) #","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"../input/rsna-2022-cervical-spine-fracture-detection/\"\ntrainCsv = \"../input/rsna-2022-cervical-spine-fracture-detection/train.csv\"\ntrain_df = pd.read_csv(trainCsv)\nprint(train_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-08-19T22:16:42.119365Z","iopub.execute_input":"2022-08-19T22:16:42.120205Z","iopub.status.idle":"2022-08-19T22:16:42.138490Z","shell.execute_reply.started":"2022-08-19T22:16:42.120166Z","shell.execute_reply":"2022-08-19T22:16:42.137566Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"            StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7\n0   1.2.826.0.1.3680043.6200                1   1   1   0   0   0   0   0\n1  1.2.826.0.1.3680043.27262                1   0   1   0   0   0   0   0\n2  1.2.826.0.1.3680043.21561                1   0   1   0   0   0   0   0\n3  1.2.826.0.1.3680043.12351                0   0   0   0   0   0   0   0\n4   1.2.826.0.1.3680043.1363                1   0   0   0   0   1   0   0\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_dicom(path):\n    \"\"\"\n    reads a dicom file and loads the image array inside it\n    inputs:\n        path: the path of the required dicom file\n    returns:\n        data: image pixel arrays\n    \"\"\"\n    img=pydicom.dcmread(path)\n    data=img.pixel_array\n    data=data-np.min(data)\n    if np.max(data) != 0:\n        data=data/np.max(data)\n    data=(data*255).astype(np.uint8)\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-08-19T22:16:43.569533Z","iopub.execute_input":"2022-08-19T22:16:43.569909Z","iopub.status.idle":"2022-08-19T22:16:43.576191Z","shell.execute_reply.started":"2022-08-19T22:16:43.569858Z","shell.execute_reply":"2022-08-19T22:16:43.574895Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def data_generator():\n    for i, study_instance in enumerate(train_df.StudyInstanceUID[:5]):\n        for dcm in os.listdir(DATA_DIR + f\"train_images/{study_instance}\"):\n            train_labels = []\n            path = DATA_DIR + f\"train_images/{study_instance}/{dcm}\"\n            print(path)\n            img = load_dicom(path)\n            \n            # resize each image into a shape of (512, 512)\n            img = np.resize(img, (512, 512))\n            #  normalize image\n            img = img / 255.0\n            # convert from gray scale to rgb, this will be helpful incase we want to use pretrained models\n            img = tf.expand_dims(img, axis=-1)\n            img = tf.image.grayscale_to_rgb(img)\n            \n            train_labels.extend([\n                train_df.loc[i, \"C1\"],\n                train_df.loc[i, \"C2\"],\n                train_df.loc[i, \"C3\"],\n                train_df.loc[i, \"C4\"],\n                train_df.loc[i, \"C5\"],\n                train_df.loc[i, \"C6\"],\n                train_df.loc[i, \"C7\"],\n                train_df.loc[i, \"patient_overall\"] # end with patient overall\n            ])\n            yield img, train_labels","metadata":{"execution":{"iopub.status.busy":"2022-08-19T22:16:44.806985Z","iopub.execute_input":"2022-08-19T22:16:44.807601Z","iopub.status.idle":"2022-08-19T22:16:44.818494Z","shell.execute_reply.started":"2022-08-19T22:16:44.807566Z","shell.execute_reply":"2022-08-19T22:16:44.817297Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"### Testing Data after generation ###","metadata":{}},{"cell_type":"code","source":"def testDataset(dataset):\n    for img, label in dataset.take(1):\n        print(img.shape)\n        print(label.shape)\n        print(label)","metadata":{"execution":{"iopub.status.busy":"2022-08-19T22:16:46.631878Z","iopub.execute_input":"2022-08-19T22:16:46.632486Z","iopub.status.idle":"2022-08-19T22:16:46.638143Z","shell.execute_reply.started":"2022-08-19T22:16:46.632448Z","shell.execute_reply":"2022-08-19T22:16:46.636984Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the dataset ##","metadata":{}},{"cell_type":"code","source":"def splitDataset(dataset, trainFactor, img_count): # here it refers to tf.dataset\n    train_dataset = dataset.take(int(trainFactor * img_count))\n    validation_dataset = dataset.take(int((1 - trainFactor)* img_count))\n    \n    return train_dataset, validation_dataset\n    ","metadata":{"execution":{"iopub.status.busy":"2022-08-19T22:16:48.641896Z","iopub.execute_input":"2022-08-19T22:16:48.642511Z","iopub.status.idle":"2022-08-19T22:16:48.647957Z","shell.execute_reply.started":"2022-08-19T22:16:48.642464Z","shell.execute_reply":"2022-08-19T22:16:48.646932Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def configure_for_performance(data):\n    data = data.cache()\n#   data = data.shuffle(buffer_size=300)\n    data = data.batch(4)\n    data = data.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-08-19T22:16:49.862812Z","iopub.execute_input":"2022-08-19T22:16:49.863438Z","iopub.status.idle":"2022-08-19T22:16:49.868826Z","shell.execute_reply.started":"2022-08-19T22:16:49.863401Z","shell.execute_reply":"2022-08-19T22:16:49.867474Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Creating the model ##","metadata":{}},{"cell_type":"code","source":"def creating_model(opt):\n    IMG_SHAPE = (512, 512, 3)\n    base_model = tf.keras.applications.EfficientNetB5(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\n    \n    base_model.trainable = False\n    inputs = tf.keras.Input(shape=IMG_SHAPE)\n    x = base_model(inputs)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    outputs = tf.keras.layers.Dense(8, activation = 'sigmoid')(x)\n    model = tf.keras.Model(inputs, outputs)\n    #model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-08-19T22:20:09.877188Z","iopub.execute_input":"2022-08-19T22:20:09.878241Z","iopub.status.idle":"2022-08-19T22:20:09.886260Z","shell.execute_reply.started":"2022-08-19T22:20:09.878192Z","shell.execute_reply":"2022-08-19T22:20:09.885205Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, BatchNormalization, Dense, Dropout, Flatten\n\n# Define Alex Net model\ndef alex_net():\n    model = Sequential()\n\n    # 1st Convolutional Layer\n    model.add(Conv2D(filters=96, input_shape=(512,512,3), kernel_size=(11,11),\\\n     strides=(4,4), padding='valid', activation=\"relu\"))\n    # Pooling \n    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n    # Batch Normalisation before passing it to the next layer\n    model.add(BatchNormalization())\n\n    # 2nd Convolutional Layer\n    model.add(Conv2D(filters=256, kernel_size=(11,11), strides=(1,1), padding='valid', activation=\"relu\"))\n    # Pooling\n    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n\n   # Batch Normalisation\n    model.add(BatchNormalization())\n\n    # 3rd Convolutional Layer\n    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid', activation=\"relu\"))\n    # Batch Normalisation\n    model.add(BatchNormalization())\n\n    # 4th Convolutional Layer\n    model.add(Conv2D(filters=384, kernel_size=(3,3), strides=(1,1), padding='valid', activation=\"relu\"))\n    # Batch Normalisation\n    model.add(BatchNormalization())\n\n    # 5th Convolutional Layer\n    model.add(Conv2D(filters=256, kernel_size=(3,3), strides=(1,1), padding='valid', activation=\"relu\"))\n    # Pooling\n    model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='valid'))\n\n    # Batch Normalisation\n    model.add(BatchNormalization())\n\n    # Passing it to a dense layer\n    model.add(Flatten())\n    # 1st Dense Layer\n    model.add(Dense(4096, input_shape=(512*512*3,), activation=\"relu\"))\n    # Add Dropout to prevent overfitting\n    model.add(Dropout(0.4))\n    # Batch Normalisation\n    model.add(BatchNormalization())\n\n    # 2nd Dense Layer\n    model.add(Dense(4096, activation=\"relu\"))\n    # Add Dropout\n    model.add(Dropout(0.4))\n    # Batch Normalisation\n    model.add(BatchNormalization())\n\n    # 3rd Dense Layer\n    model.add(Dense(1000, activation=\"relu\"))\n    # Add Dropout\n    model.add(Dropout(0.4))\n    # Batch Normalisation\n    model.add(BatchNormalization())\n\n    # Output Layer with 8 probability classes\n    model.add(Dense(8, activation=\"softmax\"))\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-08-19T22:16:52.857116Z","iopub.execute_input":"2022-08-19T22:16:52.857601Z","iopub.status.idle":"2022-08-19T22:16:53.020565Z","shell.execute_reply.started":"2022-08-19T22:16:52.857563Z","shell.execute_reply":"2022-08-19T22:16:53.019427Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"trainCsv = \"../input/rsna-2022-cervical-spine-fracture-detection/train.csv\"\ntrainImagePath = \"../input/rsna-2022-cervical-spine-fracture-detection/train_images\"\ncreDataset = cl_CreatingDataset(224, 224, 4)\ntrainImagesPathList, labels = creDataset.creatingPathList(trainImagePath, trainCsv)\nprint(\"train Image list :\", len(trainImagePath), \".....\", \"train label list : \", len(labels))\n","metadata":{"execution":{"iopub.status.busy":"2022-08-14T21:02:18.871133Z","iopub.execute_input":"2022-08-14T21:02:18.871793Z","iopub.status.idle":"2022-08-14T21:05:46.011647Z","shell.execute_reply.started":"2022-08-14T21:02:18.871754Z","shell.execute_reply":"2022-08-14T21:05:46.010225Z"}}},{"cell_type":"code","source":"# this is required because of length of dataset is not valid for generators\ndef getImageCount():\n    img_count = 0\n    for _, study_instance in enumerate(train_df.StudyInstanceUID[:5]):\n        for _ in os.listdir(DATA_DIR + f\"train_images/{study_instance}\"):\n            img_count += 1\n            \n    return img_count","metadata":{"execution":{"iopub.status.busy":"2022-08-19T22:16:56.775280Z","iopub.execute_input":"2022-08-19T22:16:56.775640Z","iopub.status.idle":"2022-08-19T22:16:56.781754Z","shell.execute_reply.started":"2022-08-19T22:16:56.775607Z","shell.execute_reply":"2022-08-19T22:16:56.780172Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n# creating the dataset\ndataset = tf.data.Dataset.from_generator(data_generator, (tf.float32, tf.int8))\nfor img, label in dataset.take(1):\n    print(img.shape)\n    print(label.shape)\n    print(label)\n\n# splitting the dataset\ntrainFactor = 0.8\nimg_count = getImageCount()\nprint(\"[***] Images for training and validation : \", img_count)\n\ntrain_data, validation_data = splitDataset(dataset, trainFactor, img_count)\ntrain_dataset = configure_for_performance(train_data)\nvalidation_dataset = configure_for_performance(validation_data)\n#print(\"[*]------Length of Trainig data : \", len(train_dataset))\n#print(\"[*]------Length of validation data : \", len(validation_dataset))\n\n# creating and compiling the model\nINIT_LR = 1e-3\nopt = tf.keras.optimizers.Adam(learning_rate=INIT_LR)\nmodel = creating_model(opt)\n#model = alex_net()\nmodel.summary()\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), \n              loss=tf.keras.losses.CategoricalCrossentropy(),\n              metrics=[tf.keras.metrics.CategoricalAccuracy()])\n# training\nEPOCHS = 10\nBATCH_SIZE = 4\n#model.fit(train_dataset, batch_size = BATCH_SIZE, epochs = EPOCHS, validation_data = validation_dataset)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-19T22:20:36.779370Z","iopub.execute_input":"2022-08-19T22:20:36.779736Z","iopub.status.idle":"2022-08-19T22:20:42.440754Z","shell.execute_reply.started":"2022-08-19T22:20:36.779703Z","shell.execute_reply":"2022-08-19T22:20:42.439701Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/240.dcm\n(512, 512, 3)\n(8,)\ntf.Tensor([1 1 0 0 0 0 0 1], shape=(8,), dtype=int8)\n[***] Images for training and validation :  1734\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 512, 512, 3)]     0         \n_________________________________________________________________\nefficientnetb5 (Functional)  (None, 16, 16, 2048)      28513527  \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 2048)              0         \n_________________________________________________________________\ndropout_3 (Dropout)          (None, 2048)              0         \n_________________________________________________________________\ndense_4 (Dense)              (None, 8)                 16392     \n=================================================================\nTotal params: 28,529,919\nTrainable params: 16,392\nNon-trainable params: 28,513,527\n_________________________________________________________________\n","output_type":"stream"}]},{"cell_type":"code","source":"model.fit(train_dataset, batch_size = BATCH_SIZE, epochs = EPOCHS, validation_data = validation_dataset)","metadata":{"execution":{"iopub.status.busy":"2022-08-19T22:20:49.485440Z","iopub.execute_input":"2022-08-19T22:20:49.486076Z","iopub.status.idle":"2022-08-19T22:21:13.722379Z","shell.execute_reply.started":"2022-08-19T22:20:49.486039Z","shell.execute_reply":"2022-08-19T22:21:13.712132Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Epoch 1/10\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/240.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/12.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/210.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/120.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/208.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/141.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/18.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/9.dcm\n      1/Unknown - 13s 13s/step - loss: 6.3229 - categorical_accuracy: 0.0000e+00../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/97.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/95.dcm\n      2/Unknown - 13s 103ms/step - loss: 6.1306 - categorical_accuracy: 0.2500  ../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/138.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/165.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/124.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/22.dcm\n      3/Unknown - 13s 139ms/step - loss: 5.9322 - categorical_accuracy: 0.3333../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/116.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/96.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/25.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/190.dcm\n      4/Unknown - 13s 151ms/step - loss: 5.7532 - categorical_accuracy: 0.3750../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/234.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/71.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/241.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/39.dcm\n      5/Unknown - 13s 159ms/step - loss: 5.5699 - categorical_accuracy: 0.3500../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/45.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/14.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/231.dcm\n      6/Unknown - 13s 161ms/step - loss: 5.4205 - categorical_accuracy: 0.3333../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/11.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/64.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/44.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/24.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/109.dcm\n      7/Unknown - 14s 164ms/step - loss: 5.2977 - categorical_accuracy: 0.3214../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/131.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/139.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/133.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/166.dcm\n      8/Unknown - 14s 166ms/step - loss: 5.1767 - categorical_accuracy: 0.3125../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/61.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/207.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/88.dcm\n      9/Unknown - 14s 168ms/step - loss: 5.0502 - categorical_accuracy: 0.33333680043.6200/147.d\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/121.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/160.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/163.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/201.dcm\n     10/Unknown - 14s 170ms/step - loss: 4.9473 - categorical_accuracy: 0.3250../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/110.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/103.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/188.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/172.dcm\n     11/Unknown - 14s 169ms/step - loss: 4.8505 - categorical_accuracy: 0.3409../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/34.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/113.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/211.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/174.dcm\n     12/Unknown - 14s 169ms/step - loss: 4.7609 - categorical_accuracy: 0.3125../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/29.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/23.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/145.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/79.dcm\n     13/Unknown - 15s 171ms/step - loss: 4.6799 - categorical_accuracy: 0.3269../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/194.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/77.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/146.dcm\n     14/Unknown - 15s 169ms/step - loss: 4.6069 - categorical_accuracy: 0.3214../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/229.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/225.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/134.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/117.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/62.dcm\n     15/Unknown - 15s 170ms/step - loss: 4.5346 - categorical_accuracy: 0.3333../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/170.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/92.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/154.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/41.dcm\n     16/Unknown - 15s 170ms/step - loss: 4.4749 - categorical_accuracy: 0.3281../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/189.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/233.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/84.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/203.dcm\n     17/Unknown - 15s 171ms/step - loss: 4.4181 - categorical_accuracy: 0.3088../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/132.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/68.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/209.dcm\n     18/Unknown - 15s 170ms/step - loss: 4.3677 - categorical_accuracy: 0.29173680043.6200/238.d\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/35.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/59.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/196.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/86.dcm\n     19/Unknown - 16s 171ms/step - loss: 4.3185 - categorical_accuracy: 0.2895../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/56.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/168.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/100.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/60.dcm\n     20/Unknown - 16s 171ms/step - loss: 4.2728 - categorical_accuracy: 0.3000../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/90.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/184.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/80.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/153.dcm\n     21/Unknown - 16s 171ms/step - loss: 4.2310 - categorical_accuracy: 0.2857../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/73.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/10.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/235.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/151.dcm\n     22/Unknown - 16s 171ms/step - loss: 4.1929 - categorical_accuracy: 0.2955../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/150.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/46.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/112.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/85.dcm\n     23/Unknown - 16s 171ms/step - loss: 4.1585 - categorical_accuracy: 0.2935../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/81.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/28.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/149.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/93.dcm\n     24/Unknown - 16s 171ms/step - loss: 4.1251 - categorical_accuracy: 0.2812../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/43.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/152.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/87.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/82.dcm\n     25/Unknown - 17s 171ms/step - loss: 4.0952 - categorical_accuracy: 0.2900../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/66.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/83.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/50.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/37.dcm\n     26/Unknown - 17s 171ms/step - loss: 4.0673 - categorical_accuracy: 0.2788../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/91.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/17.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/177.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/30.dcm\n     27/Unknown - 17s 171ms/step - loss: 4.0411 - categorical_accuracy: 0.2778../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/148.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/1.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/200.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/15.dcm\n     28/Unknown - 17s 171ms/step - loss: 4.0171 - categorical_accuracy: 0.2768../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/2.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/223.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/52.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/175.dcm\n     29/Unknown - 17s 171ms/step - loss: 3.9940 - categorical_accuracy: 0.2759../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/181.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/125.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/214.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/182.dcm\n     30/Unknown - 17s 171ms/step - loss: 3.9721 - categorical_accuracy: 0.2750../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/142.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/217.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/167.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/159.dcm\n     31/Unknown - 18s 171ms/step - loss: 3.9515 - categorical_accuracy: 0.2742../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/197.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/48.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/74.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/36.dcm\n     32/Unknown - 18s 171ms/step - loss: 3.9323 - categorical_accuracy: 0.2734../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/215.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/161.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/8.dcm\n     33/Unknown - 18s 171ms/step - loss: 3.9143 - categorical_accuracy: 0.2652../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/7.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/213.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/220.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/228.dcm\n     34/Unknown - 18s 171ms/step - loss: 3.8977 - categorical_accuracy: 0.25743680043.6200/53.d\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/55.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/70.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/119.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/129.dcm\n     35/Unknown - 18s 172ms/step - loss: 3.8815 - categorical_accuracy: 0.2571../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/89.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/21.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/179.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/75.dcm\n     36/Unknown - 19s 171ms/step - loss: 3.8662 - categorical_accuracy: 0.2569../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/158.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/101.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/205.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/173.dcm\n     37/Unknown - 19s 171ms/step - loss: 3.8517 - categorical_accuracy: 0.2500../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/204.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/78.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/94.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/49.dcm\n     38/Unknown - 19s 172ms/step - loss: 3.8380 - categorical_accuracy: 0.2566../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/51.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/33.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/105.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/162.dcm\n     39/Unknown - 19s 171ms/step - loss: 3.8249 - categorical_accuracy: 0.2564../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/69.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/5.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/126.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/199.dcm\n     40/Unknown - 19s 171ms/step - loss: 3.8127 - categorical_accuracy: 0.2562../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/4.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/58.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/192.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/195.dcm\n     41/Unknown - 19s 171ms/step - loss: 3.8009 - categorical_accuracy: 0.2561../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/171.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/42.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/206.dcm\n     42/Unknown - 20s 171ms/step - loss: 3.7896 - categorical_accuracy: 0.2500../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/224.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/65.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/135.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/54.dcm\n     43/Unknown - 20s 171ms/step - loss: 3.7787 - categorical_accuracy: 0.2500../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/108.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/111.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/216.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/128.dcm\n     44/Unknown - 20s 171ms/step - loss: 3.7685 - categorical_accuracy: 0.2443../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/63.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/115.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/155.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/130.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/98.dcm\n     45/Unknown - 20s 171ms/step - loss: 3.7588 - categorical_accuracy: 0.2444../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/99.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/236.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/198.dcm\n     46/Unknown - 20s 171ms/step - loss: 3.7491 - categorical_accuracy: 0.2446../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/47.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/183.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/31.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/102.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/137.dcm\n     47/Unknown - 20s 171ms/step - loss: 3.7399 - categorical_accuracy: 0.2447../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/218.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/157.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/118.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/67.dcm\n     48/Unknown - 21s 172ms/step - loss: 3.7310 - categorical_accuracy: 0.2396../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/38.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/222.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/202.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/19.dcm\n     49/Unknown - 21s 171ms/step - loss: 3.7225 - categorical_accuracy: 0.2398../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/104.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/57.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/27.dcm\n     50/Unknown - 21s 171ms/step - loss: 3.7145 - categorical_accuracy: 0.2350../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/156.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/219.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/143.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/6.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/76.dcm\n     51/Unknown - 21s 172ms/step - loss: 3.7068 - categorical_accuracy: 0.2402../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/122.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/227.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/16.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/20.dcm\n     52/Unknown - 21s 172ms/step - loss: 3.6993 - categorical_accuracy: 0.2404../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/187.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/40.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/237.dcm\n     53/Unknown - 21s 172ms/step - loss: 3.6920 - categorical_accuracy: 0.24533680043.6200/230.d\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/212.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/169.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/243.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/232.dcm\n     54/Unknown - 22s 172ms/step - loss: 3.6852 - categorical_accuracy: 0.2407../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/186.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/144.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/178.dcm\n     55/Unknown - 22s 172ms/step - loss: 3.6785 - categorical_accuracy: 0.2364../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/242.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/191.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/3.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/176.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/106.dcm\n     56/Unknown - 22s 172ms/step - loss: 3.6719 - categorical_accuracy: 0.2411../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/123.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/107.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/226.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/136.dcm\n     57/Unknown - 22s 172ms/step - loss: 3.6656 - categorical_accuracy: 0.2500../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/180.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/114.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/127.dcm\n     58/Unknown - 22s 173ms/step - loss: 3.6596 - categorical_accuracy: 0.2543../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/72.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/32.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/164.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/193.dcm\n     59/Unknown - 23s 173ms/step - loss: 3.6537 - categorical_accuracy: 0.2542../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/239.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/26.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/185.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/221.dcm\n     60/Unknown - 23s 174ms/step - loss: 3.6480 - categorical_accuracy: 0.2583../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/13.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.6200/140.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/377.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/257.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/240.dcm\n     61/Unknown - 23s 174ms/step - loss: 3.6380 - categorical_accuracy: 0.2623../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/317.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/12.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/404.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/323.dcm\n     62/Unknown - 23s 174ms/step - loss: 3.6150 - categorical_accuracy: 0.2621../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/210.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/327.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/253.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/120.dcm\n     63/Unknown - 23s 173ms/step - loss: 3.5926 - categorical_accuracy: 0.2659../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/325.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/208.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/380.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/141.dcm\n     64/Unknown - 23s 172ms/step - loss: 3.5710 - categorical_accuracy: 0.2695../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/18.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/9.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/97.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/338.dcm\n     65/Unknown - 23s 171ms/step - loss: 3.5500 - categorical_accuracy: 0.2769../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/95.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/138.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/346.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/249.dcm\n     66/Unknown - 24s 171ms/step - loss: 3.5296 - categorical_accuracy: 0.2765../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/165.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/244.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/264.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/124.dcm\n     67/Unknown - 24s 170ms/step - loss: 3.5098 - categorical_accuracy: 0.2836../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/22.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/403.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/116.dcm\n     68/Unknown - 24s 170ms/step - loss: 3.4906 - categorical_accuracy: 0.29043680043.27262/96.d\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/25.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/190.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/234.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/283.dcm\n     69/Unknown - 24s 169ms/step - loss: 3.4719 - categorical_accuracy: 0.2971../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/303.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/71.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/381.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/241.dcm\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_572/296138487.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    883\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    884\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 885\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    886\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    915\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3039\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3040\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1962\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1963\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1964\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1966\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    594\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m           outputs = execute.execute_with_cancellation(\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"},{"name":"stdout","text":"../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/393.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/245.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/39.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/45.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/14.dcm\n../input/rsna-2022-cervical-spine-fracture-detection/train_images/1.2.826.0.1.3680043.27262/231.dcm\n","output_type":"stream"}]},{"cell_type":"code","source":"for image, label in train_dataset.take(1):\n    imageNp = image.numpy()\n    print(imageNp.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainCsv = \"../input/rsna-2022-cervical-spine-fracture-detection/train.csv\"\ntrain_df = pd.read_csv(trainCsv)\nDATA_DIR = \"../input/rsna-2022-cervical-spine-fracture-detection/\"\nfor i, study_instance in enumerate(train_df.StudyInstanceUID[:5]):\n    for dcm in os.listdir(DATA_DIR + f\"train_images/{study_instance}\"):\n            path = DATA_DIR + f\"train_images/{study_instance}/{dcm}\"\n            print(path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}