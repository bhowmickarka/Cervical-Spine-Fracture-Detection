{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom glob import glob\nimport pydicom\nimport tensorflow as tf\nimport tqdm as tqdm\nimport tensorflow_io as tfio\nimport pathlib\nimport datetime\n# tensorboard\n%load_ext tensorboard\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\n\"\"\"\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\"\"\"\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":5.477112,"end_time":"2022-08-01T23:24:50.070980","exception":false,"start_time":"2022-08-01T23:24:44.593868","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2022-08-20T22:26:22.107328Z","iopub.execute_input":"2022-08-20T22:26:22.107716Z","iopub.status.idle":"2022-08-20T22:26:24.036881Z","shell.execute_reply.started":"2022-08-20T22:26:22.107680Z","shell.execute_reply":"2022-08-20T22:26:24.035969Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"\"\\nfor dirname, _, filenames in os.walk('/kaggle/input'):\\n    for filename in filenames:\\n        print(os.path.join(dirname, filename))\\n\""},"metadata":{}}]},{"cell_type":"code","source":"!pip install -q ../input/for-pydicom/pylibjpeg-1.4.0-py3-none-any.whl\n!pip install -q ../input/for-pydicom/python_gdcm-3.0.14-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -q ../input/for-pydicom/pylibjpeg_libjpeg-1.3.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl","metadata":{"execution":{"iopub.status.busy":"2022-08-20T22:25:49.979739Z","iopub.execute_input":"2022-08-20T22:25:49.980591Z","iopub.status.idle":"2022-08-20T22:26:18.112045Z","shell.execute_reply.started":"2022-08-20T22:25:49.980550Z","shell.execute_reply":"2022-08-20T22:26:18.110956Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Creating the Dataset tf.Dataset.from_tensor_slices #","metadata":{"papermill":{"duration":0.003234,"end_time":"2022-08-01T23:24:50.076828","exception":false,"start_time":"2022-08-01T23:24:50.073594","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class cl_CreatingDataset:\n    def __init__(self, imageHeight, imageWidth, batch_size):\n        self.imageHeight = imageHeight\n        self.imageWidth = imageWidth\n        self.batch_size = batch_size\n\n    \n    def creatingPathList(self, trainImagesPath, trainDfPath):\n        trainDf = pd.read_csv(trainDfPath)\n        trainImagesPathList = []\n        labels = []\n        # reading the tain CSV\n        trainDf = pd.read_csv(trainDfPath)\n        for i in tqdm.tqdm(range(len(trainDf))):\n            folderName = trainDf[\"StudyInstanceUID\"].iloc[i]\n            imageFolderPath = os.path.join(trainImagesPath, folderName)\n            for file in glob(os.path.join(imageFolderPath, \"*.dcm\")):\n                # taking the imageName\n                trainImagesPathList.append(file)\n                # creating the labels\n                label = np.array([trainDf[\"C1\"].iloc[i],trainDf[\"C2\"].iloc[i], trainDf[\"C3\"].iloc[i], trainDf[\"C4\"].iloc[i], trainDf[\"C5\"].iloc[i], \n                                 trainDf[\"C6\"].iloc[i], trainDf[\"C7\"].iloc[i], trainDf[\"patient_overall\"].iloc[i]])\n                labels.append(label)\n        \n        return trainImagesPathList, labels\n    \n    def parse_function(self, filename, label):\n        image_bytes = tf.io.read_file(filename)\n        image = tfio.image.decode_dicom_image(image_bytes, dtype=tf.float32)\n        #image = tf.image.convert_image_dtype(image, tf.float32)\n        resized_image = tf.image.resize(image, [self.imageHeight, self.imageWidth])\n        compressedImage = tf.squeeze(resized_image, axis = 0)\n        finalImage = tf.repeat(compressedImage, repeats=[3], axis = 2)\n        return finalImage, label\n        \n    def train_preprocess(self, image, label):\n        return image, label\n\n\n    def creatingDataset(self, filenames, labels):\n        dataset = tf.data.Dataset.from_tensor_slices((filenames, labels))\n        dataset = dataset.shuffle(len(filenames))\n        dataset = dataset.map(self.parse_function, num_parallel_calls=4)\n        dataset = dataset.map(self.train_preprocess, num_parallel_calls=4)\n        dataset = dataset.batch(self.batch_size)\n        dataset = dataset.prefetch(1)\n\n        return dataset","metadata":{"papermill":{"duration":0.01753,"end_time":"2022-08-01T23:24:50.096873","exception":false,"start_time":"2022-08-01T23:24:50.079343","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Creating Dataset using Generator Concept (tf.data.Dataset.fromgenerator) #","metadata":{}},{"cell_type":"code","source":"DATA_DIR = \"../input/rsna-2022-cervical-spine-fracture-detection/\"\ntrainCsv = \"../input/rsna-2022-cervical-spine-fracture-detection/train.csv\"\ntrain_df = pd.read_csv(trainCsv)\nprint(train_df.head())","metadata":{"execution":{"iopub.status.busy":"2022-08-20T22:26:28.197027Z","iopub.execute_input":"2022-08-20T22:26:28.198331Z","iopub.status.idle":"2022-08-20T22:26:28.215417Z","shell.execute_reply.started":"2022-08-20T22:26:28.198285Z","shell.execute_reply":"2022-08-20T22:26:28.214360Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"            StudyInstanceUID  patient_overall  C1  C2  C3  C4  C5  C6  C7\n0   1.2.826.0.1.3680043.6200                1   1   1   0   0   0   0   0\n1  1.2.826.0.1.3680043.27262                1   0   1   0   0   0   0   0\n2  1.2.826.0.1.3680043.21561                1   0   1   0   0   0   0   0\n3  1.2.826.0.1.3680043.12351                0   0   0   0   0   0   0   0\n4   1.2.826.0.1.3680043.1363                1   0   0   0   0   1   0   0\n","output_type":"stream"}]},{"cell_type":"code","source":"def load_dicom(path):\n    \"\"\"\n    reads a dicom file and loads the image array inside it\n    inputs:\n        path: the path of the required dicom file\n    returns:\n        data: image pixel arrays\n    \"\"\"\n    img=pydicom.dcmread(path)\n    data=img.pixel_array\n    data=data-np.min(data)\n    if np.max(data) != 0:\n        data=data/np.max(data)\n    data=(data*255).astype(np.uint8)\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-08-20T22:26:30.121767Z","iopub.execute_input":"2022-08-20T22:26:30.122959Z","iopub.status.idle":"2022-08-20T22:26:30.129211Z","shell.execute_reply.started":"2022-08-20T22:26:30.122913Z","shell.execute_reply":"2022-08-20T22:26:30.128042Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def data_generator():\n    for i, study_instance in enumerate(train_df.StudyInstanceUID[:]):\n        for dcm in os.listdir(DATA_DIR + f\"train_images/{study_instance}\"):\n            train_labels = []\n            path = DATA_DIR + f\"train_images/{study_instance}/{dcm}\"\n            img = load_dicom(path)\n            \n            # resize each image into a shape of (512, 512)\n            img = np.resize(img, (512, 512))\n            #  normalize image\n            img = img / 255.0\n            # convert from gray scale to rgb, this will be helpful incase we want to use pretrained models\n            img = tf.expand_dims(img, axis=-1)\n            img = tf.image.grayscale_to_rgb(img)\n            \n            train_labels.extend([\n                train_df.loc[i, \"C1\"],\n                train_df.loc[i, \"C2\"],\n                train_df.loc[i, \"C3\"],\n                train_df.loc[i, \"C4\"],\n                train_df.loc[i, \"C5\"],\n                train_df.loc[i, \"C6\"],\n                train_df.loc[i, \"C7\"],\n                train_df.loc[i, \"patient_overall\"] # end with patient overall\n            ])\n            yield img, train_labels","metadata":{"execution":{"iopub.status.busy":"2022-08-20T22:26:32.177721Z","iopub.execute_input":"2022-08-20T22:26:32.178107Z","iopub.status.idle":"2022-08-20T22:26:32.187530Z","shell.execute_reply.started":"2022-08-20T22:26:32.178055Z","shell.execute_reply":"2022-08-20T22:26:32.186247Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"## Splitting the dataset ##","metadata":{}},{"cell_type":"code","source":"def splitDataset(dataset, trainFactor, img_count): # here it refers to tf.dataset\n    train_dataset = dataset.take(int(trainFactor * img_count))\n    validation_dataset = dataset.take(int((1 - trainFactor)* img_count))\n    \n    return train_dataset, validation_dataset\n    ","metadata":{"execution":{"iopub.status.busy":"2022-08-20T22:26:34.194941Z","iopub.execute_input":"2022-08-20T22:26:34.195524Z","iopub.status.idle":"2022-08-20T22:26:34.201101Z","shell.execute_reply.started":"2022-08-20T22:26:34.195481Z","shell.execute_reply":"2022-08-20T22:26:34.200045Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def configure_for_performance(data):\n    data = data.cache()\n    data = data.batch(16)\n    data = data.prefetch(buffer_size=tf.data.AUTOTUNE)\n    return data","metadata":{"execution":{"iopub.status.busy":"2022-08-20T22:26:35.915631Z","iopub.execute_input":"2022-08-20T22:26:35.916265Z","iopub.status.idle":"2022-08-20T22:26:35.921304Z","shell.execute_reply.started":"2022-08-20T22:26:35.916228Z","shell.execute_reply":"2022-08-20T22:26:35.920244Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Creating the model ##","metadata":{}},{"cell_type":"code","source":"def creating_model(opt):\n    IMG_SHAPE = (512, 512, 3)\n    base_model = tf.keras.applications.EfficientNetB5(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')\n    \n    base_model.trainable = False\n    inputs = tf.keras.Input(shape=IMG_SHAPE)\n    x = base_model(inputs)\n    x = tf.keras.layers.GlobalAveragePooling2D()(x)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    outputs = tf.keras.layers.Dense(8, activation = 'sigmoid')(x)\n    model = tf.keras.Model(inputs, outputs)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-08-20T22:26:37.818864Z","iopub.execute_input":"2022-08-20T22:26:37.819297Z","iopub.status.idle":"2022-08-20T22:26:37.827829Z","shell.execute_reply.started":"2022-08-20T22:26:37.819260Z","shell.execute_reply":"2022-08-20T22:26:37.826556Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"trainCsv = \"../input/rsna-2022-cervical-spine-fracture-detection/train.csv\"\ntrainImagePath = \"../input/rsna-2022-cervical-spine-fracture-detection/train_images\"\ncreDataset = cl_CreatingDataset(224, 224, 4)\ntrainImagesPathList, labels = creDataset.creatingPathList(trainImagePath, trainCsv)\nprint(\"train Image list :\", len(trainImagePath), \".....\", \"train label list : \", len(labels))\n","metadata":{"execution":{"iopub.status.busy":"2022-08-14T21:02:18.871133Z","iopub.execute_input":"2022-08-14T21:02:18.871793Z","iopub.status.idle":"2022-08-14T21:05:46.011647Z","shell.execute_reply.started":"2022-08-14T21:02:18.871754Z","shell.execute_reply":"2022-08-14T21:05:46.010225Z"}}},{"cell_type":"code","source":"# this is required because of length of dataset is not valid for generators\ndef getImageCount():\n    img_count = 0\n    for _, study_instance in enumerate(train_df.StudyInstanceUID[:5]):\n        for _ in os.listdir(DATA_DIR + f\"train_images/{study_instance}\"):\n            img_count += 1\n            \n    return img_count","metadata":{"execution":{"iopub.status.busy":"2022-08-20T22:28:14.512701Z","iopub.execute_input":"2022-08-20T22:28:14.513585Z","iopub.status.idle":"2022-08-20T22:28:14.518942Z","shell.execute_reply.started":"2022-08-20T22:28:14.513546Z","shell.execute_reply":"2022-08-20T22:28:14.518010Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"def createTensorboardCallback(logdir):\n       return tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-20T22:28:16.162681Z","iopub.execute_input":"2022-08-20T22:28:16.163311Z","iopub.status.idle":"2022-08-20T22:28:16.168140Z","shell.execute_reply.started":"2022-08-20T22:28:16.163274Z","shell.execute_reply":"2022-08-20T22:28:16.167138Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n# creating the dataset\ndataset = tf.data.Dataset.from_generator(data_generator, (tf.float32, tf.int8))\n\n# printing a sample data for checking\nfor img, label in dataset.take(1):\n    print(img.shape)\n    print(label.shape)\n    print(label)\n\n# splitting the dataset\ntrainFactor = 0.8\nimg_count = getImageCount()\nprint(\"[***] Images for training and validation : \", img_count)\ntrain_data, validation_data = splitDataset(dataset, trainFactor, img_count)\n#\ntrain_dataset = configure_for_performance(train_data)\nvalidation_dataset = configure_for_performance(validation_data)\n\n# creating and compiling the model\nINIT_LR = 1e-3\nopt = tf.keras.optimizers.Adam(learning_rate=INIT_LR)\nmodel = creating_model(opt)\n#model = alex_net()\nmodel.summary()\nmodel.compile(optimizer=tf.keras.optimizers.Adam(), \n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=[tf.keras.metrics.BinaryAccuracy()])\n# training\nEPOCHS = 10\nBATCH_SIZE = 4\n# tensorboard logging dir\nfoldername = \"/kaggle/working/tensorboardRecord\"\nos.makedirs(foldername, exist_ok=True)\nlogdir = os.path.join(foldername, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n\ntensorboardCallback = createTensorboardCallback(logdir)\nmodel.fit(train_dataset, batch_size = BATCH_SIZE, epochs = EPOCHS, validation_data = validation_dataset, callbacks=[tensorboardCallback])\n","metadata":{"execution":{"iopub.status.busy":"2022-08-20T22:28:25.236515Z","iopub.execute_input":"2022-08-20T22:28:25.236997Z","iopub.status.idle":"2022-08-20T22:36:07.951163Z","shell.execute_reply.started":"2022-08-20T22:28:25.236957Z","shell.execute_reply":"2022-08-20T22:36:07.950010Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(512, 512, 3)\n(8,)\ntf.Tensor([1 1 0 0 0 0 0 1], shape=(8,), dtype=int8)\n[***] Images for training and validation :  1734\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 512, 512, 3)]     0         \n_________________________________________________________________\nefficientnetb5 (Functional)  (None, 16, 16, 2048)      28513527  \n_________________________________________________________________\nglobal_average_pooling2d (Gl (None, 2048)              0         \n_________________________________________________________________\ndropout (Dropout)            (None, 2048)              0         \n_________________________________________________________________\ndense (Dense)                (None, 8)                 16392     \n=================================================================\nTotal params: 28,529,919\nTrainable params: 16,392\nNon-trainable params: 28,513,527\n_________________________________________________________________\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/keras/utils/generic_utils.py:497: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n  category=CustomMaskWarning)\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10\n87/87 [==============================] - 79s 756ms/step - loss: 0.2623 - binary_accuracy: 0.9219 - val_loss: 0.9176 - val_binary_accuracy: 0.6622\nEpoch 2/10\n87/87 [==============================] - 35s 398ms/step - loss: 0.3391 - binary_accuracy: 0.8635 - val_loss: 0.7668 - val_binary_accuracy: 0.6622\nEpoch 3/10\n87/87 [==============================] - 35s 399ms/step - loss: 0.3151 - binary_accuracy: 0.8599 - val_loss: 0.7563 - val_binary_accuracy: 0.6622\nEpoch 4/10\n87/87 [==============================] - 34s 392ms/step - loss: 0.3139 - binary_accuracy: 0.8603 - val_loss: 0.7470 - val_binary_accuracy: 0.6622\nEpoch 5/10\n87/87 [==============================] - 34s 397ms/step - loss: 0.3137 - binary_accuracy: 0.8586 - val_loss: 0.7341 - val_binary_accuracy: 0.6622\nEpoch 6/10\n87/87 [==============================] - 35s 399ms/step - loss: 0.3106 - binary_accuracy: 0.8601 - val_loss: 0.7376 - val_binary_accuracy: 0.6622\nEpoch 7/10\n87/87 [==============================] - 35s 399ms/step - loss: 0.3106 - binary_accuracy: 0.8598 - val_loss: 0.7295 - val_binary_accuracy: 0.6622\nEpoch 8/10\n87/87 [==============================] - 35s 399ms/step - loss: 0.3083 - binary_accuracy: 0.8606 - val_loss: 0.7331 - val_binary_accuracy: 0.6622\nEpoch 9/10\n87/87 [==============================] - 35s 399ms/step - loss: 0.3099 - binary_accuracy: 0.8602 - val_loss: 0.7298 - val_binary_accuracy: 0.6622\nEpoch 10/10\n87/87 [==============================] - 35s 398ms/step - loss: 0.3111 - binary_accuracy: 0.8600 - val_loss: 0.7307 - val_binary_accuracy: 0.6622\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"<keras.callbacks.History at 0x7f31843b7890>"},"metadata":{}}]},{"cell_type":"code","source":"%tensorboard --logdir /kaggle/working/tensorboardRecord","metadata":{"execution":{"iopub.status.busy":"2022-08-20T22:38:47.275023Z","iopub.execute_input":"2022-08-20T22:38:47.275788Z","iopub.status.idle":"2022-08-20T22:38:47.288490Z","shell.execute_reply.started":"2022-08-20T22:38:47.275753Z","shell.execute_reply":"2022-08-20T22:38:47.287306Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Reusing TensorBoard on port 6006 (pid 4054), started 0:01:43 ago. (Use '!kill 4054' to kill it.)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n      <iframe id=\"tensorboard-frame-5ac6270b05fd786a\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-5ac6270b05fd786a\");\n          const url = new URL(\"/\", window.location);\n          const port = 6006;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "},"metadata":{}}]},{"cell_type":"code","source":"rm -rf /kaggle/working/","metadata":{"execution":{"iopub.status.busy":"2022-08-20T22:25:24.535624Z","iopub.execute_input":"2022-08-20T22:25:24.536294Z","iopub.status.idle":"2022-08-20T22:25:25.528288Z","shell.execute_reply.started":"2022-08-20T22:25:24.536206Z","shell.execute_reply":"2022-08-20T22:25:25.527112Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"rm: cannot remove '/kaggle/working/': Device or resource busy\n","output_type":"stream"}]}]}